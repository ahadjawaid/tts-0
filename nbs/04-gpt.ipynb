{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ea609",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffae298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.68, -0.97,  0.98],\n",
       "        [ 0.75, -1.10,  0.63],\n",
       "        [ 0.23,  2.03,  1.22]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn((3,3)); t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2110b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones_like(t)); mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3798d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.68,  -inf,  -inf],\n",
       "        [ 0.75, -1.10,  -inf],\n",
       "        [ 0.23,  2.03,  1.22]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.masked_fill(mask==0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0cfa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029dd74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(query: Tensor, key: Tensor, value: Tensor, mask: bool = False) -> Tensor:\n",
    "    key_dim = key.shape[-1]\n",
    "\n",
    "    scores = (query @ key.transpose(-1, -2)) / sqrt(key_dim)\n",
    "    \n",
    "    if mask:\n",
    "        scores_mask = torch.tril(torch.ones_like(scores))\n",
    "        scores = scores.masked_fill(scores_mask==0, float('-inf'))\n",
    "        \n",
    "    return F.softmax(scores, dim=-1) @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, query, value = [torch.randn((3,3)) for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff24d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.68, -0.87,  1.06],\n",
       "        [-0.17, -0.43,  0.60],\n",
       "        [-0.76, -0.92,  1.10]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product(key, query, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f72255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.69,  0.42, -0.51],\n",
       "        [ 0.09, -0.13,  0.14],\n",
       "        [-0.76, -0.92,  1.10]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product(key, query, value, mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff7cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, channels: int, mask: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(channels, channels*3, bias=False)\n",
    "        self.mask = mask\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        h = self.linear(x)\n",
    "        query, key, value = torch.chunk(h, chunks=3, dim=-1)\n",
    "        \n",
    "        attention_scores = scaled_dot_product(query, key, value, self.mask)\n",
    "        \n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35fc62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc1c34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.48, -0.11,  0.01],\n",
       "        [-0.57, -0.20,  0.02],\n",
       "        [-0.66, -0.29,  0.03]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AttentionHead(channels=3, mask=False)(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ddcdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.87, -0.60, -0.61],\n",
       "        [ 0.45, -0.36, -0.30],\n",
       "        [ 0.45, -0.36, -0.30]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AttentionHead(channels=3, mask=True)(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, n_heads: int, mask: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.attention_heads = nn.ModuleList([AttentionHead(in_channels, mask) for _ in range(n_heads)])\n",
    "        \n",
    "        self.linear = nn.Linear(in_channels*n_heads, out_channels)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        h = torch.cat([head(x) for head in self.attention_heads], dim=-1)\n",
    "        \n",
    "        return self.linear(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ed329",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn((3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d362d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.22,  0.14, -0.29],\n",
       "        [ 0.24,  0.14, -0.28],\n",
       "        [ 0.24,  0.12, -0.28]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiHeadAttention(in_channels=3, out_channels=3, n_heads=3)(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0bc7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.68, -0.04,  0.03],\n",
       "        [ 0.31,  0.17, -0.09],\n",
       "        [ 0.27,  0.10, -0.13]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiHeadAttention(in_channels=3, out_channels=3, n_heads=3, mask=True)(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a939883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(in_channels: int, out_channels: int, Activation: Module = None, init: bool = True) -> Module:\n",
    "    layers = [nn.Linear(in_channels, out_channels)]\n",
    "    \n",
    "    if Activation:\n",
    "        layers.append(Activation())\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c981d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fcff8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.00, 0.00, 0.00],\n",
       "        [0.00, 0.39, 0.00],\n",
       "        [0.00, 0.00, 0.13]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(3, 3, nn.ReLU)(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 hidden_channels: int, \n",
    "                 out_channels: int, \n",
    "                 n_hidden: int = 0, \n",
    "                 p_dropout: float = 0.,\n",
    "                 Activation=nn.ReLU) -> None:\n",
    "        super().__init__()\n",
    "        self.in_layer = linear(in_channels, hidden_channels, Activation)\n",
    "        \n",
    "        self.hidden = nn.Sequential(*[linear(hidden_channels, hidden_channels, Activation) \n",
    "                                      for _ in range(n_hidden)])\n",
    "        \n",
    "        self.out_layer = linear(hidden_channels, out_channels, Activation=None)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        h = self.in_layer(x)\n",
    "        h = self.hidden(h)\n",
    "        h = self.out_layer(h)\n",
    "        return self.dropout(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5927df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn((3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464d34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.26, -0.19, -0.10],\n",
       "        [ 0.20, -0.17, -0.07],\n",
       "        [ 0.17, -0.19, -0.03]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FeedForward(in_channels=3, hidden_channels=10, out_channels=3, n_hidden=2)(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091880a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_relu_init(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9873e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward = FeedForward(in_channels=3, hidden_channels=10, out_channels=3, n_hidden=2).apply(apply_relu_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bfa453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.82, -1.62, -0.80],\n",
       "        [-0.54, -2.96, -1.40],\n",
       "        [-0.66, -1.98, -0.33]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 hidden_channels: int, \n",
    "                 out_channels: int, \n",
    "                 n_heads: int, \n",
    "                 n_hidden_layers: int = 0,\n",
    "                 p_dropout: float = 0.,\n",
    "                 mask: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm((in_channels, hidden_channels))\n",
    "        self.norm2 = nn.LayerNorm((hidden_channels, out_channels))\n",
    "        \n",
    "        self.attention = MultiHeadAttention(in_channels, hidden_channels, n_heads, mask)\n",
    "        self.feed_forward = FeedForward(hidden_channels, hidden_channels, out_channels, n_hidden_layers, p_dropout)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        h = self.attention(self.norm1(x)) + x\n",
    "        return self.feed_forward(self.norm2(h)) + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransformerLayer(in_channels=3, \n",
    "                           hidden_channels=3, \n",
    "                           out_channels=3, \n",
    "                           n_heads=3, \n",
    "                           n_hidden_layers=0,\n",
    "                           p_dropout=0.1,\n",
    "                           mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f61f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.64, -0.69, -1.81],\n",
       "        [ 0.29,  0.60,  0.26],\n",
       "        [ 0.67, -1.11, -0.89]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a3ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook:\n",
    "    def __init__(self, module: Module, func: Callable) -> None:\n",
    "        self.hook = module.register_forward_hook(partial(func, self))\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.remove()\n",
    "        \n",
    "    def __enter__(self, *args):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.remove()\n",
    "    \n",
    "    def remove(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderBlock(in_channels=3, hidden_channels=3, out_channels=3, n_heads=3, n_hidden_layers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2769cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = []\n",
    "prev = None\n",
    "for i, module in enumerate(decoder.modules()):\n",
    "    if isinstance(module, nn.ReLU) and i != 0:\n",
    "        modules.append((module, prev))\n",
    "    prev = module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d459acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, module in modules:\n",
    "    apply_relu_init(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1398b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.84,  1.38, -3.20],\n",
       "        [ 0.82,  0.02, -0.82],\n",
       "        [ 0.59, -1.21, -0.89]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae84229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_sz: int, hidden_channels: int) -> None:\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
