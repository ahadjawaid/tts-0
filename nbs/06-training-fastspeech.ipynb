{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f01e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp fastspeech_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd402bd3",
   "metadata": {},
   "source": [
    "# Fastspeech Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ded9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tk541/mambaforge/envs/torch/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/tk541/mambaforge/envs/torch/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <7702F607-92FA-3D67-9D09-0710D936B85A> /Users/tk541/mambaforge/envs/torch/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <1E9FA061-EA31-3736-81D0-79A33B965097> /Users/tk541/mambaforge/envs/torch/lib/python3.10/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from tts_0.fastspeech import FastSpeech\n",
    "from tts_0.tts_data import TTSDataset, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from tts_0.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b0501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "from tts_0.learn import *\n",
    "from typing import Callable, Sequence\n",
    "from torch.nn import Module\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import fastcore.all as fc\n",
    "from torch import tensor\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torch\n",
    "from fastspeech.visualize import show_mel\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2dbada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainFastSpeech(Learner):\n",
    "    def __init__(self, model: Module, dls: DataLoaders, loss_fn: Callable, loss_fn_b: Callable, lr: float = 0.1, \n",
    "                 cbs: Sequence[Callback] = (), optimizer_fn: Callable = optim.SGD) -> None:\n",
    "        super().__init__(model, dls, loss_fn, lr, cbs, optimizer_fn)\n",
    "        self.loss_fn_b = loss_fn_b\n",
    "        \n",
    "    def predict(self) -> None:\n",
    "        phones, durations = self.batch[:2]\n",
    "\n",
    "        self.preds = self.model(phones, durations)\n",
    "    \n",
    "    def get_loss(self) -> None:\n",
    "        d_slice = (slice(None), slice(None, -1))\n",
    "        durations, mels, mel_alignment, duration_alignment = self.batch[1:]\n",
    "        bs = mels.shape[0]\n",
    "        \n",
    "        self.loss, self.duration_loss = tensor(0.), tensor(0.)\n",
    "        for i in range(bs):\n",
    "            mel_slice = (slice(None), slice(None, mel_alignment[i]))\n",
    "            duration_slice = slice(None, duration_alignment[i])\n",
    "            \n",
    "            self.loss += self.loss_fn(self.preds[0][i][mel_slice], mels[i][mel_slice]) / bs\n",
    "            self.duration_loss += self.loss_fn_b(self.preds[1][i][duration_slice], \n",
    "                                                 durations[i][duration_slice].to(torch.float)) / bs\n",
    "    \n",
    "    def backward(self) -> None:\n",
    "        self.loss.backward()\n",
    "        self.duration_loss.backward()\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5489084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ScheduleLR(Callback):\n",
    "    def __init__(self, Scheduler, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.Scheduler = Scheduler\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "    def before_fit(self, learn):\n",
    "        learn.scheduler = self.Scheduler(learn.optimizer, *self.args, **self.kwargs)\n",
    "        \n",
    "    def after_batch(self, learn):\n",
    "        learn.scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd31630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ShowMelCB(Callback):\n",
    "    def __init__(self, xb, n_steps=500):\n",
    "        super().__init__()\n",
    "        fc.store_attr()\n",
    "        self.i = 0\n",
    "    \n",
    "    def _show_mel(self):\n",
    "        with torch.no_grad():\n",
    "            phones, durations = self.xb[:2]\n",
    "\n",
    "            mel, _ = learn.model(phones, durations)\n",
    "        \n",
    "        show_mel(to_cpu(mel)[0])\n",
    "        \n",
    "    def after_batch(self, learn):\n",
    "        if self.i % self.n_steps == 0:\n",
    "            self._show_mel()\n",
    "        self.i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f15b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Hook:\n",
    "    def __init__(self, module: Module, func: Callable) -> None:\n",
    "        self.hook = module.register_forward_hook(partial(func, self))\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.remove()\n",
    "        \n",
    "    def __enter__(self, *args):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.remove()\n",
    "    \n",
    "    def remove(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ce86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def colorize_mel(mel):\n",
    "    db = librosa.power_to_db(mel)\n",
    "    norm_mel = normalize_image(db)\n",
    "    \n",
    "    cmap = matplotlib.colormaps['viridis']\n",
    "    \n",
    "    return cmap(norm_mel)[::-1]\n",
    "\n",
    "def normalize(image):\n",
    "    return (image - np.min(image)) / (np.max(image) - np.min(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f949bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mel_to_audio(mel: Tensor, sr: int, n_fft: int, hop_length: int, n_iter: int = 32):\n",
    "    return librosa.feature.inverse.mel_to_audio(mel.numpy() if isinstance(mel, Tensor) else mel, \n",
    "                                                sr=sr, \n",
    "                                                n_fft=n_fft, \n",
    "                                                hop_length=hop_length, \n",
    "                                                n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b30217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class WandBCB(MetricsCB):\n",
    "    count=100\n",
    "    def __init__(self, \n",
    "                 config: dict, \n",
    "                 project: str, \n",
    "                 *ms, \n",
    "                 xb: Tensor = None, \n",
    "                 mel_to_audio: Callable = None,\n",
    "                 sample_rate: int = 22050,\n",
    "                 **metrics) -> None:\n",
    "        fc.store_attr()\n",
    "        super().__init__(*ms, **metrics)\n",
    "        \n",
    "    def _log(self, d: dict, *args, **kwargs):        \n",
    "        if self.xb:\n",
    "            mels = self.sample_mels()\n",
    "            \n",
    "            wandb.log({ \"Mel-Spectrograms\": [wandb.Image(colorize_mel(mel)) for mel in mels] })\n",
    "            \n",
    "            if self.mel_to_audio:\n",
    "                audios = list(map(partial(wandb.Audio, sample_rate=self.sample_rate), \n",
    "                                  map(self.mel_to_audio, mels)))\n",
    "                \n",
    "                wandb.log({ \"Audio\": audios })\n",
    "            \n",
    "    def sample_mels(self):\n",
    "        with torch.no_grad():\n",
    "            phones, durations = self.xb[:2]\n",
    "\n",
    "            mels, _ = to_cpu(learn.model(phones, durations))\n",
    "        \n",
    "        return to_cpu(mels)\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        wandb.init(project=self.project, config=self.config)\n",
    "        wandb.watch(learn.model)\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        wandb.log({\"loss\": learn.loss})\n",
    "    \n",
    "    def after_fit(self, learn):\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233da85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def count_parameters(model):\n",
    "    return sum([param.numel() for param in list(model.parameters())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "path = '../../data/LJSpeech-1.1/wavs/'\n",
    "path_vocab = '../../fastspeech/sample_data/cmudict-0.7b.symbols.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "sample_rate = 22050\n",
    "hop_length = 256\n",
    "n_bins = 80\n",
    "n_fft = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a201a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d9a82c9989484a96e4ab46f4970e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13082 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = TTSDataset(path, path_vocab, sample_rate, n_fft, hop_length, n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5faff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "batch_size = 12\n",
    "n_workers = 0\n",
    "persistent_workers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5ff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, \n",
    "                batch_size, \n",
    "                shuffle=True, \n",
    "                collate_fn=partial(collate_fn, pad_num=ds.vocab.pad_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bcb7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "xb = next(iter(dl))\n",
    "dls = DataLoaders(dl, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"mps\" if torch.backends.mps.is_available() else\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    embedding_size = len(ds.vocab),\n",
    "    hidden_channels = 32,\n",
    "    filter_channels = 128,\n",
    "    out_channels = n_bins,\n",
    "    ks = 3,\n",
    "    n_heads = 2,\n",
    "    n_hidden = 0,\n",
    "    p_dropout = 0.1,\n",
    "    n_encoders = 1,\n",
    "    n_decoder = 1,\n",
    "    Activation = nn.ReLU,\n",
    "    init_fn = partial(nn.init.kaiming_normal_, nonlinearity='relu'),\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175f342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2182"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "n_epochs = 2\n",
    "n_batches = len(dls.train)\n",
    "n_steps = n_batches * n_epochs\n",
    "n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa6a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88385"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastSpeech(**config)\n",
    "parameters = count_parameters(model)\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TrainFastSpeech(model, dls, F.l1_loss, F.mse_loss, cbs=[]).lr_find(start_lr=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97578ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 3e-1\n",
    "project_name = \"fastspeech\"\n",
    "\n",
    "mel_to_audio_preset = partial(mel_to_audio, sr=sample_rate, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "wandbcb = WandBCB(project=project_name, \n",
    "                  xb=xb,\n",
    "                  sample_rate=sample_rate, \n",
    "                  mel_to_audio=mel_to_audio_preset, \n",
    "                  config={\n",
    "                        \"max_lr\": max_lr,\n",
    "                        \"n_epochs\": n_epochs,\n",
    "                        \"n_batches\": n_batches,\n",
    "                        \"n_steps\": n_steps,\n",
    "                        \"parameters\": parameters,\n",
    "                        **config\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693cc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahadjawaid0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tk541/programming/ml/torch/tts-0/nbs/wandb/run-20230803_072808-0i58kmcj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ahadjawaid0/fastspeech/runs/0i58kmcj' target=\"_blank\">celestial-resonance-10</a></strong> to <a href='https://wandb.ai/ahadjawaid0/fastspeech' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ahadjawaid0/fastspeech' target=\"_blank\">https://wandb.ai/ahadjawaid0/fastspeech</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ahadjawaid0/fastspeech/runs/0i58kmcj' target=\"_blank\">https://wandb.ai/ahadjawaid0/fastspeech/runs/0i58kmcj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cbs = [DeviceCB(device), ScheduleLR(OneCycleLR, max_lr, n_steps), wandbcb]\n",
    "learn = TrainFastSpeech(model, dls, F.l1_loss, F.mse_loss, lr=max_lr, cbs=cbs)\n",
    "learn.fit(n_epochs, valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291e2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
