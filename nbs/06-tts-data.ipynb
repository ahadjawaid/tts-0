{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b478c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tts_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb64375",
   "metadata": {},
   "source": [
    "# TTS-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dddc08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from fastspeech.loading import *\n",
    "from fastspeech.preprocess import *\n",
    "from torch import tensor\n",
    "import fastcore.all as fc\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6be4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "path = '../../data/LJSpeech-1.1/wavs/'\n",
    "path_vocab = '../../fastspeech/sample_data/cmudict-0.7b.symbols.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3044d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "file_paths = list(map(str, get_audio_files(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965560bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "sampling_rate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TTSDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 path_data: str,\n",
    "                 path_vocab: str,\n",
    "                 sr: int = 22050,\n",
    "                 n_fft: int = 1024,\n",
    "                 hl: int = 256,\n",
    "                 nb: int = 80,\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        replace_to_tg = partial(replace_extension, extension=\".TextGrid\")\n",
    "        \n",
    "        files = get_audio_files(path_data)\n",
    "        files_tg = files.map(replace_to_tg)\n",
    "        \n",
    "        self.vocab = Vocab(path_vocab, [\"spn\"])\n",
    "        \n",
    "        self.data = []\n",
    "        for audio_file, tg_file in tqdm(list(zip(files, files_tg))):\n",
    "            wav = load_audio(audio_file, sr=sr)\n",
    "            mel = melspectrogram(wav, n_fft=n_fft, hl=hl, nb=nb)\n",
    "            phones, duration = get_phones_and_durations(tg_file, sr, hl)\n",
    "            \n",
    "            phones = tensor(phones_list_to_num(phones, self.vocab)).squeeze()\n",
    "            mels = tensor(melspectrogram(wav, n_fft, hl, nb))\n",
    "            durations = round_and_align_durations(tensor(duration), mels.shape[-1]).to(torch.int)\n",
    "            \n",
    "            self.data.append((phones, durations, mels))\n",
    "            \n",
    "    def __getitem__(self, index: int) -> tuple:\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f344f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5cc887d2f743a39e6bb292faa02faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13082 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "ds = TTSDataset(path, path_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa45fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([27,  6, 69, 67, 38, 54, 81, 45, 66, 80, 30, 55, 26, 33, 22,  9, 67, 85,\n",
       "         52, 38, 54, 69, 77, 27,  9, 65, 34, 83,  9, 55, 70, 66, 59, 55, 27,  9,\n",
       "         24,  7, 24,  9, 53, 59, 55, 48,  9, 55, 82, 59, 65,  9, 55, 53, 48, 66,\n",
       "         44, 24, 30, 53, 26]),\n",
       " tensor([ 3, 11,  8,  9, 15,  7,  8, 10, 17,  7,  5,  6,  5, 17, 10,  9,  7, 75,\n",
       "          9, 12,  4,  6,  3,  3,  4,  9, 11,  5,  4,  6,  8,  7, 18, 13, 46,  3,\n",
       "          4,  8,  5,  2,  8,  9,  3,  8,  3,  3,  8, 17,  8,  4,  5,  4,  4,  9,\n",
       "          4, 10, 13, 15,  9], dtype=torch.int32),\n",
       " tensor([[3.7324e-06, 4.3111e-05, 4.4483e-05,  ..., 3.1676e-07, 3.7787e-07,\n",
       "          3.6053e-06],\n",
       "         [2.0968e-05, 8.7105e-05, 2.5985e-05,  ..., 4.2691e-06, 3.1428e-06,\n",
       "          1.3746e-05],\n",
       "         [6.8580e-05, 1.2529e-03, 4.5900e-03,  ..., 3.4039e-05, 1.2055e-05,\n",
       "          1.7352e-05],\n",
       "         ...,\n",
       "         [7.6227e-08, 7.9915e-07, 1.8581e-05,  ..., 1.1654e-07, 7.4758e-08,\n",
       "          3.9378e-08],\n",
       "         [4.1749e-08, 2.1261e-06, 1.0184e-04,  ..., 5.6812e-08, 6.2095e-08,\n",
       "          5.5092e-08],\n",
       "         [2.9916e-08, 4.3956e-07, 2.0438e-05,  ..., 1.6961e-08, 2.1799e-08,\n",
       "          2.2520e-08]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def collate_fn(inp, pad_num: int):\n",
    "    phones, durations, mels = [item[0] for item in inp], [item[1] for item in inp], [item[2] for item in inp]\n",
    "    \n",
    "    mel_attention = tensor(list(map(lambda t: t.shape[-1], mels)))\n",
    "    phones_attention = tensor(list(map(lambda t: t.shape[-1], phones)))\n",
    "    \n",
    "    mel_batched = pad_mels(mels, 0)\n",
    "    phones_batched = pad_phones(pad_max_seq(phones), pad_num)\n",
    "    mel_len = mel_batched.shape[-1]\n",
    "    \n",
    "    duration_batched = pad_duration(pad_max_seq(durations), mel_batched.shape[-1])\n",
    "    \n",
    "    assert phones_batched.shape == duration_batched.shape\n",
    "    assert len(duration_batched.sum(dim=1).unique()) == 1\n",
    "    \n",
    "    return phones_batched, duration_batched, mel_batched, mel_attention, phones_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514666e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "pad_num = ds.vocab.pad_num\n",
    "dl = DataLoader(ds, 2, shuffle=True, collate_fn=partial(collate_fn, pad_num=pad_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad6c085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[27, 45, 67, 45, 82,  9, 67, 65, 30, 68, 53, 48, 55, 30, 67,  9, 67, 31,\n",
       "          66, 48, 80, 45, 27, 66,  9, 41,  2, 66, 26, 69, 77, 27,  9, 30, 40, 24,\n",
       "          49, 22,  9, 55, 26, 67, 49, 22, 38, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
       "          84, 84],\n",
       "         [27,  9, 51, 10, 51, 44, 55, 65,  6, 67, 44, 56, 67, 30, 55, 69,  9, 55,\n",
       "          67, 10, 79, 26, 30, 70, 69, 59, 53, 26, 42, 45, 54, 42, 49, 66, 45, 25,\n",
       "          53, 48, 26, 44, 82, 34, 79, 26, 27,  9, 65, 10, 55, 44, 68, 54,  9, 55,\n",
       "          69,  0]]),\n",
       " tensor([[ 6, 10, 13, 16,  7,  4, 10,  6,  6,  9, 11,  5,  8,  6,  9,  4, 11,  9,\n",
       "          10,  9,  7,  4,  5, 10,  4,  7,  6, 10,  6,  3,  4,  4, 10, 12, 13,  3,\n",
       "          11, 23, 10,  4,  4, 16, 11, 17, 31,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0, 73],\n",
       "         [ 4,  4, 10, 19, 10,  5,  7,  5, 11, 10,  4,  5, 10,  8,  4,  5,  5,  5,\n",
       "           7,  4, 10,  6, 19, 17, 59, 12,  3,  9,  1,  6,  5,  5,  9, 13,  4, 19,\n",
       "           5,  7,  6,  4,  8, 18,  8,  4,  2,  3, 11,  7,  4,  3, 14,  3,  6,  5,\n",
       "          20,  0]], dtype=torch.int32),\n",
       " tensor([[[5.3249e-05, 4.0191e-05, 1.8337e-05,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [1.3999e-04, 3.9495e-04, 4.0311e-04,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [1.4809e-03, 6.0149e-03, 6.7546e-03,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          ...,\n",
       "          [2.0075e-08, 2.3618e-08, 2.2817e-08,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.3473e-09, 1.2120e-08, 1.6009e-08,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [3.2556e-09, 4.7464e-09, 5.0503e-09,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "         [[5.6224e-06, 4.9731e-06, 2.0143e-05,  ..., 5.8955e-07,\n",
       "           6.0038e-07, 9.1877e-06],\n",
       "          [2.2771e-05, 9.0167e-05, 4.2686e-04,  ..., 3.3087e-05,\n",
       "           1.1688e-04, 1.5825e-04],\n",
       "          [2.9584e-05, 1.0537e-04, 2.1709e-03,  ..., 1.1999e-04,\n",
       "           3.6414e-04, 3.6386e-04],\n",
       "          ...,\n",
       "          [3.0819e-07, 9.7549e-06, 1.0546e-04,  ..., 2.4106e-07,\n",
       "           2.0162e-07, 1.8323e-07],\n",
       "          [1.1973e-06, 2.3286e-05, 1.1355e-04,  ..., 1.2414e-07,\n",
       "           1.2607e-07, 9.7983e-08],\n",
       "          [1.0095e-06, 8.3504e-06, 2.6462e-05,  ..., 1.3841e-08,\n",
       "           2.2336e-08, 2.2680e-08]]]),\n",
       " tensor([404, 477]),\n",
       " tensor([45, 55]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14272c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
