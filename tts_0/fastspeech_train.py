# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06-training-fastspeech.ipynb.

# %% auto 0
__all__ = ['TrainFastSpeech', 'ScheduleLR', 'ShowMelCB', 'Hook', 'count_parameters']

# %% ../nbs/06-training-fastspeech.ipynb 2
from .fastspeech import FastSpeech
from .tts_data import TTSDataset, collate_fn
from torch.utils.data import DataLoader
from functools import partial
from .data import *

# %% ../nbs/06-training-fastspeech.ipynb 3
import torch.nn as nn
from .learn import *
from typing import Callable, Sequence
from torch.nn import Module
from torch import optim
import torch.nn.functional as F
import fastcore.all as fc
from torch import tensor
from torch.optim.lr_scheduler import OneCycleLR
import torch
from fastspeech.visualize import show_mel

# %% ../nbs/06-training-fastspeech.ipynb 4
class TrainFastSpeech(Learner):
    def __init__(self, model: Module, dls: DataLoaders, loss_fn: Callable, loss_fn_b: Callable, lr: float = 0.1, 
                 cbs: Sequence[Callback] = (), optimizer_fn: Callable = optim.SGD) -> None:
        super().__init__(model, dls, loss_fn, lr, cbs, optimizer_fn)
        self.loss_fn_b = loss_fn_b
        
    def predict(self) -> None:
        phones, durations = self.batch[:2]

        self.preds = self.model(phones, durations)
    
    def get_loss(self) -> None:
        d_slice = (slice(None), slice(None, -1))
        durations, mels, mel_alignment, duration_alignment = self.batch[1:]
        bs = mels.shape[0]
        
        self.loss, self.duration_loss = tensor(0.), tensor(0.)
        for i in range(bs):
            mel_slice = (slice(None), slice(None, mel_alignment[i]))
            duration_slice = slice(None, duration_alignment[i])
            
            self.loss += self.loss_fn(self.preds[0][i][mel_slice], mels[i][mel_slice]) / bs
            self.duration_loss += self.loss_fn_b(self.preds[1][i][duration_slice], 
                                                 durations[i][duration_slice].to(torch.float)) / bs
    
    def backward(self) -> None:
        self.loss.backward()
        self.duration_loss.backward()
    
    def step(self) -> None:
        self.optimizer.step()
    
    def zero_grad(self) -> None:
        self.optimizer.zero_grad()

# %% ../nbs/06-training-fastspeech.ipynb 5
class ScheduleLR(Callback):
    def __init__(self, Scheduler, *args, **kwargs):
        super().__init__()
        self.Scheduler = Scheduler
        self.args = args
        self.kwargs = kwargs
        
    def before_fit(self, learn):
        learn.scheduler = self.Scheduler(learn.optimizer, *self.args, **self.kwargs)
        
    def after_batch(self, learn):
        learn.scheduler.step()

# %% ../nbs/06-training-fastspeech.ipynb 6
class ShowMelCB(Callback):
    def __init__(self, xb, n_steps=500):
        super().__init__()
        fc.store_attr()
        self.i = 0
    
    def _show_mel(self):
        with torch.no_grad():
            phones, durations = self.xb[:2]

            mel, _ = learn.model(phones, durations)
        
        show_mel(to_cpu(mel)[0])
        
    def after_batch(self, learn):
        if self.i % self.n_steps == 0:
            self._show_mel()
        self.i += 1

# %% ../nbs/06-training-fastspeech.ipynb 7
class Hook:
    def __init__(self, module: Module, func: Callable) -> None:
        self.hook = module.register_forward_hook(partial(func, self))
    
    def __del__(self):
        self.remove()
        
    def __enter__(self, *args):
        return self
    
    def __exit__(self, *args):
        self.remove()
    
    def remove(self):
        self.hook.remove()

# %% ../nbs/06-training-fastspeech.ipynb 8
def count_parameters(model):
    return sum([param.numel() for param in list(model.parameters())])
