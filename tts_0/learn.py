# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01-learn.ipynb.

# %% auto 0
__all__ = ['Callback', 'CancelFitException', 'CancelEpochException', 'CancelBatchException', 'WithCB', 'LRFinderCB', 'Learner',
           'TrainLearner', 'CountBatchsCB', 'SingleBatchCB', 'TrainCB', 'DeviceCB', 'MetricsCB', 'ProgressCB']

# %% ../nbs/01-learn.ipynb 1
from torcheval.metrics import MulticlassAccuracy, Mean
from typing import Any, Callable, Sequence, Mapping
from torch.optim.lr_scheduler import ExponentialLR
from fastprogress import progress_bar, master_bar
from operator import attrgetter
import matplotlib.pyplot as plt
from functools import partial
from torch.nn import Module
import fastcore.all as fc
from .data import *
from torch import optim
from copy import copy
import math
import torch

# %% ../nbs/01-learn.ipynb 7
class Callback:
    order = 0

# %% ../nbs/01-learn.ipynb 8
class CancelFitException(Exception): pass
class CancelEpochException(Exception): pass
class CancelBatchException(Exception): pass

# %% ../nbs/01-learn.ipynb 9
class WithCB:
    def __init__(self, name: str) -> None:
        self.name = name
    
    def __call__(self, fn: Callable) -> Callable:
        def _fn(obj: object, *args, **kwargs):
            try:
                obj.callback(f"before_{self.name}")
                fn(obj, *args, **kwargs)
                obj.callback(f"after_{self.name}")  
            except globals()[f"Cancel{self.name.capitalize()}Exception"]:
                pass
            finally:
                obj.callback(f"cleanup_{self.name}")
                
        return _fn

# %% ../nbs/01-learn.ipynb 10
class LRFinderCB(Callback):
    def __init__(self, gamma: float = 1.3, max_mult: float = 2.5) -> None:
        fc.store_attr()
    
    def before_fit(self, learn) -> None:
        self.losses, self.lrs, self.min = [], [], math.inf
        learn.scheduler = ExponentialLR(learn.optimizer, self.gamma)
    
    def after_batch(self, learn) -> None:
        if not learn.training:
            raise CancelEpochException()
        
        lr = learn.optimizer.param_groups[0]['lr']
        loss = to_cpu(learn.loss)
        
        self.lrs.append(lr)
        self.losses.append(loss)
        
        self.min = min(self.min, loss)
        if math.isnan(loss) or loss > self.min*self.max_mult:
            raise CancelFitException()
        
        learn.scheduler.step()
    
    def cleanup_fit(self, learn) -> None:
        plt.plot(self.lrs, self.losses)
        plt.xscale("log")
        plt.xlabel("Learning Rate")
        plt.ylabel("Loss")

# %% ../nbs/01-learn.ipynb 11
class Learner:
    def __init__(self, model: Module, dls: DataLoaders, loss_fn: Callable, lr: float = 0.1, 
                 cbs: Sequence[Callback] = (), optimizer_fn: Callable = optim.SGD) -> None:
        fc.store_attr()
    
    def __getattr__(self, attribute) -> Callable:
        if attribute in ["predict", "get_loss", "backward", "step", "zero_grad"]:
            return partial(self.callback, attribute)
        raise AttributeError(attribute)
    
    @property
    def training(self) -> bool:
        return self.model.training
    
    def callback(self, method_name: str) -> None:
        self._run_callback(self.cbs, method_name)
    
    def _run_callback(self, cbs: Sequence[Callback], method_name: str) -> None:
        for cb in sorted(cbs, key=attrgetter("order")):
            method = getattr(cb, method_name, None)
            if method is not None: method(self)
                
    def fit(self, n_epochs: int, train: bool = True, valid: bool = True, 
            cbs: Sequence[Callback] = (), lr: float = None) -> None:
        for cb in cbs: self.cbs.append(cb)
        try:
            self.n_epochs = n_epochs
            self.epochs = range(n_epochs)
            
            if lr is None: lr = self.lr
                
            if self.optimizer_fn: 
                self.optimizer = self.optimizer_fn(self.model.parameters(), lr)
            
            self._fit(train, valid)            
        finally:
            for cb in cbs: self.cbs.remove(cb)
    
    @WithCB("fit")
    def _fit(self, train: bool, valid: bool) -> None:
        for self.epoch in self.epochs:
            if train: self.one_epoch(training=True)
            if valid: torch.no_grad()(self.one_epoch)(training=False)
    
    def one_epoch(self, training: bool) -> None:
        self.model.train(training)
        self.dl = self.dls.train if training else self.dls.valid
        self._one_epoch()
    
    @WithCB("epoch")
    def _one_epoch(self) -> None:
        for self.iter, self.batch in enumerate(self.dl):
            self._one_batch()
    
    @WithCB("batch")
    def _one_batch(self) -> None:
        self.predict()
        self.callback("after_predict")
        self.get_loss()
        self.callback("after_loss")
        if self.training:
            self.backward()
            self.callback("after_backward")
            self.step()
            self.callback("after_step")
            self.zero_grad()
            
    def lr_find(self, gamma: float = 1.3, max_mult: float = 2.5, 
                start_lr: float = 1e-5, max_epochs: int = 10):
        self.fit(max_epochs, train=True, valid=False, cbs=[LRFinderCB(gamma, max_mult)], 
                 lr=start_lr)

# %% ../nbs/01-learn.ipynb 12
class TrainLearner(Learner):
    def predict(self) -> None:
        x = self.batch[0]
        self.preds = self.model(x)
    
    def get_loss(self) -> None:
        y = self.batch[1]
        self.loss = self.loss_fn(self.preds, y)
    
    def backward(self) -> None:
        self.loss.backward()
    
    def step(self) -> None:
        self.optimizer.step()
    
    def zero_grad(self) -> None:
        self.optimizer.zero_grad()

# %% ../nbs/01-learn.ipynb 14
class CountBatchsCB(Callback):
    count = 0
    def after_batch(self, learn: Learner) -> None:
        self.count += 1
    
    def cleanup_fit(self, learn: Learner) -> None:
        print(f"Number of Batches: {self.count}")

# %% ../nbs/01-learn.ipynb 16
class SingleBatchCB(Callback):
    def after_batch(self, learn: Learner) -> None:
        raise CancelFitException()

# %% ../nbs/01-learn.ipynb 18
class TrainCB(Callback):
    def __init__(self, n_inputs: int = 1) -> None:
        self.n_inputs = n_inputs
    
    def predict(self, learn: Learner) -> None:
        x = learn.batch[:self.n_inputs]
        learn.preds = learn.model(*x)
    
    def get_loss(self, learn: Learner) -> None:
        y = learn.batch[self.n_inputs:]
        learn.loss = learn.loss_fn(learn.preds, *y)
    
    def backward(self, learn: Learner) -> None:
        learn.loss.backward()
    
    def step(self, learn: Learner) -> None:
        learn.optimizer.step()
    
    def zero_grad(self, learn: Learner) -> None:
        learn.optimizer.zero_grad()

# %% ../nbs/01-learn.ipynb 20
class DeviceCB(Callback):
    def __init__(self, device: str = def_device) -> None:
        self.device = device
    
    def before_fit(self, learn: Learner) -> None:
        if hasattr(learn.model, "to"):
            learn.model.to(self.device)
    
    def before_batch(self, learn: Learner) -> None:
        learn.batch = to_device(learn.batch, device=self.device)

# %% ../nbs/01-learn.ipynb 22
class MetricsCB(Callback):
    def __init__(self, *ms, **metrics) -> None:
        for item in ms:
            metrics[type(item).__name__] = item
    
        self.metrics = metrics
        self.all_metrics = copy(metrics)
        self.all_metrics["loss"] = self.loss = Mean()
    
    def _log(self, d: dict, *args, **kwargs) -> None:
        print(d)
    
    def before_fit(self, learn: Learner) -> None:
        learn.metrics = self
    
    def before_epoch(self, learn: Learner) -> None:
        for metric in self.all_metrics.values():
            metric.reset()
    
    def after_batch(self, learn: Learner) -> None:
        x, y, *_ = learn.batch
        
        for metric in self.metrics.values():
            metric.update(to_cpu(learn.preds), y)
        
        self.loss.update(to_cpu(learn.loss), weight=len(x))
    
    def after_epoch(self, learn: Learner) -> None:
        log = {key: f"{value.compute():.3f}" for key, value in self.all_metrics.items()}
        log["epoch"] = learn.epoch
        log["type"] = "train" if learn.training else "eval"
        
        self._log(log, learn=learn)

# %% ../nbs/01-learn.ipynb 24
class ProgressCB(Callback):
    order = MetricsCB.order + 1
    
    def __init__(self, plot: bool = False) -> None:
        self.plot = plot
    
    def _log(self, d: dict, *args, **kwargs) -> None:
        if self.first:
            self.mbar.write(list(d), table=True)
            self.first = False
            
        self.mbar.write(list(d.values()), table=True)
    
    def before_fit(self, learn: Learner) -> None:
        if hasattr(learn, "metrics"):
            learn.metrics._log = self._log
        
        learn.epochs = self.mbar = master_bar(learn.epochs)
        self.losses, self.valid_losses = [], []
        self.first = True
    
    def before_epoch(self, learn: Learner) -> None:
        learn.dl = progress_bar(learn.dl, leave=True, parent=self.mbar) 

    def after_batch(self, learn: Learner) -> None:
        if self.plot and hasattr(learn, 'metrics') and learn.training:
            self.losses.append(learn.loss.item())
            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])

        learn.dl.comment = f'{learn.loss:.3f}'
